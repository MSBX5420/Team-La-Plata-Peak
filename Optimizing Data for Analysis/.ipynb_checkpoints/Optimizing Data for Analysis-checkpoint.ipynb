{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from IPython.display import display, Markdown\n",
    "import seaborn as sns\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import re\n",
    "from time import sleep"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read in credentials file\n",
    "This is a .txt file where I copy the access keys that are provided for me in the Account Details part of AWS classroom. This has not be included in our github repo in order to keep my keys private. The file is read in and the proper information is parsed out and used to connect to AWS Command Line Interface (CLI).\n",
    "\n",
    "\n",
    "![my_image](images/Screen Shot 2020-04-14 at 2.04.12 PM.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "key_list = []\n",
    "\n",
    "with open('/Users/kaegan/Documents/CU Boulder/2Masters/3Spring Semester/3Unstruct Dist Data/aws_credentials/credentials.txt', 'r') as file:\n",
    "    for line in file:\n",
    "        key_list.append(line.strip(\"\\n\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Parse relevant info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "access_key_id = re.search('.*=(.*)', key_list[1]).group(1)\n",
    "secret_access_key = re.search('.*=(.*)', key_list[2]).group(1)\n",
    "session_token = re.search('.*=(.*=)', key_list[3]).group(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize boto3 client with proper credentials\n",
    "s3 = boto3.client('s3',\n",
    "                aws_access_key_id=access_key_id,\n",
    "                aws_secret_access_key=secret_access_key,\n",
    "                aws_session_token=session_token,)\n",
    "glue = boto3.client('glue',\n",
    "                   aws_access_key_id=access_key_id,\n",
    "                   aws_secret_access_key=secret_access_key,\n",
    "                   aws_session_token=session_token,)\n",
    "athena = boto3.client('athena',\n",
    "                   aws_access_key_id=access_key_id,\n",
    "                   aws_secret_access_key=secret_access_key,\n",
    "                   aws_session_token=session_token,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def list_glue_databases():\n",
    "    glue_database = glue.get_databases()\n",
    "\n",
    "    for db in glue_database['DatabaseList']:\n",
    "        print(db['Name'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nycgreenyellow\n",
      "test_db\n",
      "yellowdb\n"
     ]
    }
   ],
   "source": [
    "list_glue_databases()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def list_glue_tables(database, verbose=True):\n",
    "    glue_tables = glue.get_tables(DatabaseName=database)\n",
    "    \n",
    "    for table in glue_tables['TableList']:\n",
    "        display(Markdown('**Table: ' + table['Name'] + '**'))\n",
    "        display(Markdown('Location: ' + table['StorageDescriptor']['Location']))\n",
    "        created = table['CreatedBy'].split('/')\n",
    "        display(Markdown('Created by: ' + created[-1]))\n",
    "        if verbose and created[-1] == 'AWS Crawler':\n",
    "            display(Markdown(f'Records: {int(table[\"Parameters\"][\"recordCount\"]):,}'))\n",
    "            display(Markdown(f'Average Record Size: {table[\"Parameters\"][\"averageRecordSize\"]} Bytes'))\n",
    "            display(Markdown(f'Dataset Size: {float(table[\"Parameters\"][\"sizeKey\"])/1024/1024:3.0f} MB'))\n",
    "            display(Markdown(f'Crawler: {table[\"Parameters\"][\"UPDATED_BY_CRAWLER\"]}'))\n",
    "        if verbose:\n",
    "            df_columns = pd.DataFrame.from_dict(table[\"StorageDescriptor\"][\"Columns\"])\n",
    "            display(df_columns[['Name', 'Type']])\n",
    "            display(Markdown('---'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "**Table: yellow2019**"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "Location: s3://la-plata-peak/yellow2019/"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "Created by: AWS-Crawler"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "list_glue_tables('yellowdb', verbose=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# This was giving the 404 error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def athena_query(query, bucket, folder):\n",
    "#     output = 's3://' + bucket + '/' + folder + '/'\n",
    "#     response = athena.start_query_execution(QueryString=query, \n",
    "#                                         ResultConfiguration={'OutputLocation': output})\n",
    "#     qid = response['QueryExecutionId']\n",
    "#     response = athena.get_query_execution(QueryExecutionId=qid)\n",
    "#     state = response['QueryExecution']['Status']['State']\n",
    "#     while state == 'RUNNING':\n",
    "#         response = athena.get_query_execution(QueryExecutionId=qid)\n",
    "#         state = response['QueryExecution']['Status']['State']\n",
    "#     key = folder + '/' + qid + '.csv'\n",
    "#     data_source = {'Bucket': bucket, 'Key': key}\n",
    "#     print(data_source)\n",
    "#     url = s3.generate_presigned_url(ClientMethod = 'get_object', Params = data_source)\n",
    "#     print(url)\n",
    "#     data = pd.read_csv(url)\n",
    "#     return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# bucket = 'la-plata-peak'\n",
    "# folder = 'queries'\n",
    "# query = 'SELECT * FROM \"yellowdb\".\"yellow2019\" TABLESAMPLE BERNOULLI(100) LIMIT 1000;'\n",
    "\n",
    "# df = athena_query(query, bucket, folder)\n",
    "# df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Rewrite function into two parts\n",
    "Shortly after an s3 object has been created it’ll return a 404 error if it hasn’t been copied across multiple servers yet so I had to break the function up into parts.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Perform query and save result in query folder of s3 bucket, return the key to the query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def athena_query_key(query, bucket, folder):\n",
    "    output = 's3://' + bucket + '/' + folder + '/'\n",
    "    response = athena.start_query_execution(QueryString=query, \n",
    "                                        ResultConfiguration={'OutputLocation': output})\n",
    "    qid = response['QueryExecutionId']\n",
    "    response = athena.get_query_execution(QueryExecutionId=qid)\n",
    "    state = response['QueryExecution']['Status']['State']\n",
    "    while state == 'RUNNING':\n",
    "        response = athena.get_query_execution(QueryExecutionId=qid)\n",
    "        state = response['QueryExecution']['Status']['State']\n",
    "    key = folder + '/' + qid + '.csv'\n",
    "    return key\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'queries/f494a488-f312-4cdb-8353-2c7cbbab11ed.csv'"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bucket = 'la-plata-peak'\n",
    "folder = 'queries'\n",
    "query = 'SELECT * FROM \"yellowdb\".\"yellow2019\" TABLESAMPLE BERNOULLI(100) LIMIT 1000;'\n",
    "\n",
    "key = athena_query_key(query, bucket, folder)\n",
    "key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sleep(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use key returned above to generate presigned url and access query result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def athena_get_query(bucket, key):\n",
    "    data_source = {'Bucket': bucket, 'Key': key}\n",
    "    print(data_source)\n",
    "    url = s3.generate_presigned_url(ClientMethod = 'get_object', Params = data_source)\n",
    "    print(url)\n",
    "    data = pd.read_csv(url)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Bucket': 'la-plata-peak', 'Key': 'queries/f494a488-f312-4cdb-8353-2c7cbbab11ed.csv'}\n",
      "https://la-plata-peak.s3.amazonaws.com/queries/f494a488-f312-4cdb-8353-2c7cbbab11ed.csv?AWSAccessKeyId=ASIAZU2WEI3JDXG7UO4N&Signature=oqG2DIXnCgWiabwGij2mYoFkNLs%3D&x-amz-security-token=FwoGZXIvYXdzEE0aDFe9zzN01854yKvXcyLIAYT6CkX%2FjMbdA4L5ywzFCwoBIpjqiTLhDgtSnE3FqMQUANXAONEvxLDmmAs2DYaJYpLr7UCcdWx149U3z3oWCMyT7zn8G2LdtCS3PXR69lqEmANRPc1rKxcUjwdq80p26iGLrY%2F5i3Jw3xASRxMpEIBJ8FXIv%2BO2LsKIzSvMi1Yn6osn%2FIXoYB30xK7aW6qVdMmn46jgagq9wLfxhBIH%2FLM2mr629UfJNFSOqREOdHwIc7NhBQnQ7vSrAfZllL53asJWv5K7R2DUKJzTh%2FUFMi3BAVdgkNm0SxxHYDKRmVT2B3BLbJgusPSTrjN2GfGyP%2BfUf8JUhocycKjoGxY%3D&Expires=1587679920\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>vendorid</th>\n",
       "      <th>tpep_pickup_datetime</th>\n",
       "      <th>tpep_dropoff_datetime</th>\n",
       "      <th>passenger_count</th>\n",
       "      <th>trip_distance</th>\n",
       "      <th>ratecodeid</th>\n",
       "      <th>store_and_fwd_flag</th>\n",
       "      <th>pulocationid</th>\n",
       "      <th>dolocationid</th>\n",
       "      <th>payment_type</th>\n",
       "      <th>fare_amount</th>\n",
       "      <th>extra</th>\n",
       "      <th>mta_tax</th>\n",
       "      <th>tip_amount</th>\n",
       "      <th>tolls_amount</th>\n",
       "      <th>improvement_surcharge</th>\n",
       "      <th>total_amount</th>\n",
       "      <th>congestion_surcharge</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2</td>\n",
       "      <td>2019-09-12 23:31:57</td>\n",
       "      <td>2019-09-13 00:03:14</td>\n",
       "      <td>1</td>\n",
       "      <td>6.14</td>\n",
       "      <td>1</td>\n",
       "      <td>N</td>\n",
       "      <td>161</td>\n",
       "      <td>256</td>\n",
       "      <td>1</td>\n",
       "      <td>25.5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>5.86</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.3</td>\n",
       "      <td>35.16</td>\n",
       "      <td>2.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>2019-09-12 23:30:45</td>\n",
       "      <td>2019-09-12 23:40:53</td>\n",
       "      <td>1</td>\n",
       "      <td>1.21</td>\n",
       "      <td>1</td>\n",
       "      <td>N</td>\n",
       "      <td>186</td>\n",
       "      <td>164</td>\n",
       "      <td>2</td>\n",
       "      <td>8.0</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.3</td>\n",
       "      <td>11.80</td>\n",
       "      <td>2.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>2019-09-12 23:43:58</td>\n",
       "      <td>2019-09-13 00:03:44</td>\n",
       "      <td>5</td>\n",
       "      <td>2.55</td>\n",
       "      <td>1</td>\n",
       "      <td>N</td>\n",
       "      <td>158</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>12.5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.3</td>\n",
       "      <td>16.30</td>\n",
       "      <td>2.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>2019-09-12 23:02:30</td>\n",
       "      <td>2019-09-12 23:03:18</td>\n",
       "      <td>1</td>\n",
       "      <td>0.00</td>\n",
       "      <td>2</td>\n",
       "      <td>N</td>\n",
       "      <td>75</td>\n",
       "      <td>75</td>\n",
       "      <td>3</td>\n",
       "      <td>52.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.3</td>\n",
       "      <td>52.80</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>2019-09-12 23:09:16</td>\n",
       "      <td>2019-09-12 23:21:21</td>\n",
       "      <td>1</td>\n",
       "      <td>2.60</td>\n",
       "      <td>1</td>\n",
       "      <td>N</td>\n",
       "      <td>211</td>\n",
       "      <td>170</td>\n",
       "      <td>1</td>\n",
       "      <td>11.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.5</td>\n",
       "      <td>2.95</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.3</td>\n",
       "      <td>17.75</td>\n",
       "      <td>2.5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   vendorid tpep_pickup_datetime tpep_dropoff_datetime  passenger_count  \\\n",
       "0         2  2019-09-12 23:31:57   2019-09-13 00:03:14                1   \n",
       "1         2  2019-09-12 23:30:45   2019-09-12 23:40:53                1   \n",
       "2         2  2019-09-12 23:43:58   2019-09-13 00:03:44                5   \n",
       "3         1  2019-09-12 23:02:30   2019-09-12 23:03:18                1   \n",
       "4         1  2019-09-12 23:09:16   2019-09-12 23:21:21                1   \n",
       "\n",
       "   trip_distance  ratecodeid store_and_fwd_flag  pulocationid  dolocationid  \\\n",
       "0           6.14           1                  N           161           256   \n",
       "1           1.21           1                  N           186           164   \n",
       "2           2.55           1                  N           158             4   \n",
       "3           0.00           2                  N            75            75   \n",
       "4           2.60           1                  N           211           170   \n",
       "\n",
       "   payment_type  fare_amount  extra  mta_tax  tip_amount  tolls_amount  \\\n",
       "0             1         25.5    0.5      0.5        5.86           0.0   \n",
       "1             2          8.0    0.5      0.5        0.00           0.0   \n",
       "2             2         12.5    0.5      0.5        0.00           0.0   \n",
       "3             3         52.0    0.0      0.5        0.00           0.0   \n",
       "4             1         11.0    3.0      0.5        2.95           0.0   \n",
       "\n",
       "   improvement_surcharge  total_amount  congestion_surcharge  \n",
       "0                    0.3         35.16                   2.5  \n",
       "1                    0.3         11.80                   2.5  \n",
       "2                    0.3         16.30                   2.5  \n",
       "3                    0.3         52.80                   0.0  \n",
       "4                    0.3         17.75                   2.5  "
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = athena_get_query(bucket, key)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# List out Bucket Contents if Necessary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s3_resource = boto3.resource('s3',\n",
    "                aws_access_key_id=access_key_id,\n",
    "                aws_secret_access_key=secret_access_key,\n",
    "                aws_session_token=session_token,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def list_bucket_contents(bucket, match='', size_mb=0):\n",
    "    bucket_resource = s3_resource.Bucket(bucket)\n",
    "    total_size_gb = 0\n",
    "    total_files = 0\n",
    "    match_size_gb = 0\n",
    "    match_files = 0\n",
    "    for key in bucket_resource.objects.all():\n",
    "        key_size_mb = key.size/1024/1024\n",
    "        total_size_gb += key_size_mb\n",
    "        total_files += 1\n",
    "        list_check = False\n",
    "        if not match:\n",
    "            list_check = True\n",
    "        elif match in key.key:\n",
    "            list_check = True\n",
    "        if list_check and not size_mb:\n",
    "            match_files += 1\n",
    "            match_size_gb += key_size_mb\n",
    "            print(f'{key.key} ({key_size_mb:3.0f}MB)')\n",
    "        elif list_check and key_size_mb <= size_mb:\n",
    "            match_files += 1\n",
    "            match_size_gb += key_size_mb\n",
    "            print(f'{key.key} ({key_size_mb:3.0f}MB)')\n",
    "\n",
    "    if match:\n",
    "        print(f'Matched file size is {match_size_gb/1024:3.1f}GB with {match_files} files')            \n",
    "    \n",
    "    print(f'Bucket {bucket} total size is {total_size_gb/1024:3.1f}GB with {total_files} files')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_bucket_contents(bucket='la-plata-peak', match='2019')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
