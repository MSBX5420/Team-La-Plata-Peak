{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from IPython.display import display, Markdown\n",
    "import seaborn as sns\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read in credentials file\n",
    "This is a .txt file where I copy the access keys that are provided for me in the Account Details part of AWS classroom. This has not be included in our github repo in order to keep my keys private. The file is read in and the proper information is parsed out and used to connect to AWS Command Line Interface (CLI).\n",
    "\n",
    "\n",
    "![my_image](images/Screen Shot 2020-04-14 at 2.04.12 PM.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "key_list = []\n",
    "\n",
    "with open('/Users/kaegan/Documents/CU Boulder/2Masters/3Spring Semester/3Unstruct Dist Data/aws_credentials/credentials.txt', 'r') as file:\n",
    "    for line in file:\n",
    "        key_list.append(line.strip(\"\\n\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Parse relevant info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "access_key_id = re.search('.*=(.*)', key_list[1]).group(1)\n",
    "secret_access_key = re.search('.*=(.*)', key_list[2]).group(1)\n",
    "session_token = re.search('.*=(.*=)', key_list[3]).group(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize boto3 client with proper credentials\n",
    "s3 = boto3.client('s3',\n",
    "                aws_access_key_id=access_key_id,\n",
    "                aws_secret_access_key=secret_access_key,\n",
    "                aws_session_token=session_token,)\n",
    "glue = boto3.client('glue',\n",
    "                   aws_access_key_id=access_key_id,\n",
    "                   aws_secret_access_key=secret_access_key,\n",
    "                   aws_session_token=session_token,)\n",
    "athena = boto3.client('athena',\n",
    "                   aws_access_key_id=access_key_id,\n",
    "                   aws_secret_access_key=secret_access_key,\n",
    "                   aws_session_token=session_token,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def list_glue_databases():\n",
    "    glue_database = glue.get_databases()\n",
    "\n",
    "    for db in glue_database['DatabaseList']:\n",
    "        print(db['Name'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nycgreenyellow\n"
     ]
    }
   ],
   "source": [
    "list_glue_databases()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def list_glue_tables(database, verbose=True):\n",
    "    glue_tables = glue.get_tables(DatabaseName=database)\n",
    "    \n",
    "    for table in glue_tables['TableList']:\n",
    "        display(Markdown('**Table: ' + table['Name'] + '**'))\n",
    "        display(Markdown('Location: ' + table['StorageDescriptor']['Location']))\n",
    "        created = table['CreatedBy'].split('/')\n",
    "        display(Markdown('Created by: ' + created[-1]))\n",
    "        if verbose and created[-1] == 'AWS Crawler':\n",
    "            display(Markdown(f'Records: {int(table[\"Parameters\"][\"recordCount\"]):,}'))\n",
    "            display(Markdown(f'Average Record Size: {table[\"Parameters\"][\"averageRecordSize\"]} Bytes'))\n",
    "            display(Markdown(f'Dataset Size: {float(table[\"Parameters\"][\"sizeKey\"])/1024/1024:3.0f} MB'))\n",
    "            display(Markdown(f'Crawler: {table[\"Parameters\"][\"UPDATED_BY_CRAWLER\"]}'))\n",
    "        if verbose:\n",
    "            df_columns = pd.DataFrame.from_dict(table[\"StorageDescriptor\"][\"Columns\"])\n",
    "            display(df_columns[['Name', 'Type']])\n",
    "            display(Markdown('---'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "**Table: la_plata_peak**"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "Location: s3://la-plata-peak/"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "Created by: AWS-Crawler"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "list_glue_tables('nycgreenyellow', verbose=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def athena_query(query, bucket, folder):\n",
    "    output = 's3://' + bucket + '/' + folder + '/'\n",
    "    response = athena.start_query_execution(QueryString=query, \n",
    "                                        ResultConfiguration={'OutputLocation': output})\n",
    "    qid = response['QueryExecutionId']\n",
    "    response = athena.get_query_execution(QueryExecutionId=qid)\n",
    "    state = response['QueryExecution']['Status']['State']\n",
    "    while state == 'RUNNING':\n",
    "        response = athena.get_query_execution(QueryExecutionId=qid)\n",
    "        state = response['QueryExecution']['Status']['State']\n",
    "    key = folder + '/' + qid + '.csv'\n",
    "    data_source = {'Bucket': bucket, 'Key': key}\n",
    "    print(data_source)\n",
    "    url = s3.generate_presigned_url(ClientMethod = 'get_object', Params = data_source)\n",
    "    print(url)\n",
    "    data = pd.read_csv(url)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# bucket = 'la-plata-peak'\n",
    "# folder = 'queries'\n",
    "# query = 'SELECT * FROM \"nycgreenyellow\".\"la_plata_peak\" TABLESAMPLE BERNOULLI(100) LIMIT 1000;'\n",
    "\n",
    "# df = athena_query(query, bucket, folder)\n",
    "# df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "s3_resource = boto3.resource('s3',\n",
    "                aws_access_key_id=access_key_id,\n",
    "                aws_secret_access_key=secret_access_key,\n",
    "                aws_session_token=session_token,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def list_bucket_contents(bucket, match='', size_mb=0):\n",
    "    bucket_resource = s3_resource.Bucket(bucket)\n",
    "    total_size_gb = 0\n",
    "    total_files = 0\n",
    "    match_size_gb = 0\n",
    "    match_files = 0\n",
    "    for key in bucket_resource.objects.all():\n",
    "        key_size_mb = key.size/1024/1024\n",
    "        total_size_gb += key_size_mb\n",
    "        total_files += 1\n",
    "        list_check = False\n",
    "        if not match:\n",
    "            list_check = True\n",
    "        elif match in key.key:\n",
    "            list_check = True\n",
    "        if list_check and not size_mb:\n",
    "            match_files += 1\n",
    "            match_size_gb += key_size_mb\n",
    "            print(f'{key.key} ({key_size_mb:3.0f}MB)')\n",
    "        elif list_check and key_size_mb <= size_mb:\n",
    "            match_files += 1\n",
    "            match_size_gb += key_size_mb\n",
    "            print(f'{key.key} ({key_size_mb:3.0f}MB)')\n",
    "\n",
    "    if match:\n",
    "        print(f'Matched file size is {match_size_gb/1024:3.1f}GB with {match_files} files')            \n",
    "    \n",
    "    print(f'Bucket {bucket} total size is {total_size_gb/1024:3.1f}GB with {total_files} files')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "green2019/green_tripdata_2019-09.csv ( 39MB)\n",
      "green2019/green_tripdata_2019-10.csv ( 42MB)\n",
      "green2019/green_tripdata_2019-11.csv ( 39MB)\n",
      "green2019/green_tripdata_2019-12.csv ( 39MB)\n",
      "yellow2019/yellow_tripdata_2019-09.csv (581MB)\n",
      "yellow2019/yellow_tripdata_2019-10.csv (638MB)\n",
      "yellow2019/yellow_tripdata_2019-11.csv (608MB)\n",
      "yellow2019/yellow_tripdata_2019-12.csv (610MB)\n",
      "Matched file size is 2.5GB with 8 files\n",
      "Bucket la-plata-peak total size is 2.5GB with 9 files\n"
     ]
    }
   ],
   "source": [
    "list_bucket_contents(bucket='la-plata-peak', match='2019')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
